{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IbWw8Y2feZC"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "D3OVFEQXpgdr",
        "outputId": "c0fce899-d4f2-415e-fd13-ec642b096749"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ruta del archivo\n",
        "file_path = 'imgs.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzIQIgssuo4Z",
        "outputId": "2f5c1016-621b-4e29-b00d-0c035e95968c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1032it [17:58,  1.05s/it]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "images_dir = 'imagenes_descargadas'\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "def descargar_imagen(url, sku, index):\n",
        "    \"\"\"Descarga una imagen y la guarda con un nombre basado en el SKU y el índice.\"\"\"\n",
        "    img_path = os.path.join(images_dir, f\"{sku}_{index}.jpg\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(img_path, 'wb') as file:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    file.write(chunk)\n",
        "        return img_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error descargando {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "imagenes_dict = {}\n",
        "\n",
        "max_iters = len(df)\n",
        "idx = 0\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "    if idx >= max_iters:\n",
        "        break\n",
        "    idx += 1\n",
        "    sku = row['SKU']\n",
        "    imagenes_dict[sku] = []\n",
        "    for i in range(1, 30):\n",
        "        url = row.get(f'Imagen {i}')\n",
        "        if pd.notna(url):  # Si la URL existe\n",
        "            img_path = descargar_imagen(url, sku, i)\n",
        "            if img_path:\n",
        "                imagenes_dict[sku].append(img_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZIXX-PT5fQb"
      },
      "source": [
        "## Preprocess image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUKodNcIwM7_",
        "outputId": "4ed0f74b-4393-4b19-da6b-37be687e11cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error procesando imagenes_descargadas\\1069073693_5.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1069073693_5.jpg'\n",
            "Error procesando imagenes_descargadas\\1069073693_13.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1069073693_13.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_2.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_2.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_3.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_3.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_4.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_4.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_7.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_7.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_8.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_8.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_10.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_10.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_18.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_18.jpg'\n",
            "Error procesando imagenes_descargadas\\1093284182_19.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1093284182_19.jpg'\n",
            "Error procesando imagenes_descargadas\\1149121776_1.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1149121776_1.jpg'\n",
            "Error procesando imagenes_descargadas\\1117222566_2.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1117222566_2.jpg'\n",
            "Error procesando imagenes_descargadas\\1128446717_1.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1128446717_1.jpg'\n",
            "Error procesando imagenes_descargadas\\1128446717_2.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1128446717_2.jpg'\n",
            "Error procesando imagenes_descargadas\\1128446717_5.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1128446717_5.jpg'\n",
            "Error procesando imagenes_descargadas\\1128446717_7.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1128446717_7.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_1.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_1.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_2.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_2.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_4.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_4.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_5.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_5.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_6.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_6.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_7.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_7.jpg'\n",
            "Error procesando imagenes_descargadas\\1129520406_8.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\1129520406_8.jpg'\n",
            "Error procesando imagenes_descargadas\\9949615985_3.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pabol\\\\Downloads\\\\imagenes_descargadas\\\\9949615985_3.jpg'\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def preprocesar_imagen(img_path, size=(224, 224)):\n",
        "    \"\"\"Redimensiona la imagen a un tamaño uniforme.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = img.resize(size)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "imagenes_procesadas = {sku: [preprocesar_imagen(img) for img in paths] for sku, paths in imagenes_dict.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTeJhwwfiqs"
      },
      "source": [
        "# Model and embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uqxeQiIwSJN",
        "outputId": "df9e5d9f-fa78-4fcf-ca5a-60826c09369b"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# Cargar el modelo CLIP y el procesador\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def obtener_embedding(imagen):\n",
        "    \"\"\"Genera el embedding de una imagen.\"\"\"\n",
        "    inputs = processor(images=imagen, return_tensors=\"pt\")\n",
        "    \n",
        "    # Mover los tensores a la CPU y evitar que ocupen demasiada memoria\n",
        "    with torch.no_grad():  # Desactiva el cálculo del gradiente para ahorrar memoria\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "\n",
        "    # Normalizar el embedding\n",
        "    embedding = outputs / outputs.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Liberar memoria explícitamente después de procesar la imagen\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()  # Por precaución, aunque estamos en CPU\n",
        "    \n",
        "    return embedding\n",
        "\n",
        "def procesar_en_lotes(imagenes, batch_size=8):\n",
        "    \"\"\"Procesa las imágenes en lotes para evitar problemas de memoria.\"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(imagenes), batch_size):\n",
        "        batch = imagenes[i:i + batch_size]\n",
        "        embeddings.extend([obtener_embedding(img) for img in batch if img])\n",
        "    return embeddings\n",
        "\n",
        "# Generar los embeddings por SKU\n",
        "embeddings_dict = {\n",
        "    sku: procesar_en_lotes(imgs) for sku, imgs in imagenes_procesadas.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTDuPwVvfarw"
      },
      "source": [
        "# Visualizar duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJyPm3ykw24U",
        "outputId": "6916c2de-a4bb-4e06-a869-686d14119b99"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def detectar_duplicados(embeddings, umbral=1.0):\n",
        "    \"\"\"Calcula la similitud coseno entre embeddings y devuelve pares de imágenes duplicadas.\"\"\"\n",
        "    duplicados = []\n",
        "    n = len(embeddings)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            # Convertir a numpy después de separar del cálculo de gradientes\n",
        "            similitud = cosine_similarity(\n",
        "                embeddings[i].detach().cpu().numpy(),\n",
        "                embeddings[j].detach().cpu().numpy()\n",
        "            ).item()\n",
        "            if similitud >= 1:  # Umbral de similitud\n",
        "                duplicados.append((i, j, similitud))\n",
        "    return duplicados\n",
        "\n",
        "\n",
        "duplicados_dict = {sku: detectar_duplicados(embs) for sku, embs in embeddings_dict.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ouOTwzggqa"
      },
      "source": [
        "# Eliminar duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zp_ToxhmeARZ"
      },
      "outputs": [],
      "source": [
        "def obtener_embeddings_no_duplicados(embeddings_dict, duplicados_dict):\n",
        "    \"\"\"Genera un nuevo diccionario de embeddings sin imágenes duplicadas.\"\"\"\n",
        "    # Inicializar un nuevo diccionario para las imágenes sin duplicados\n",
        "    embeddings_no_duplicados = {}\n",
        "\n",
        "    for sku, embeddings in embeddings_dict.items():\n",
        "        # Inicializar lista para almacenar embeddings únicos\n",
        "        embeddings_unicos = []\n",
        "        # Crear un conjunto para mantener los índices que se mantienen\n",
        "        indices_mantenidos = set()\n",
        "\n",
        "        # Recorrer duplicados para identificar qué índices se deben eliminar\n",
        "        for _, j, _ in duplicados_dict.get(sku, []):\n",
        "            indices_mantenidos.add(j)\n",
        "\n",
        "        # Recorrer los embeddings originales y seleccionar solo los que no están en indices_mantenidos\n",
        "        for i in range(len(embeddings)):\n",
        "            if i not in indices_mantenidos:\n",
        "                embeddings_unicos.append(embeddings[i])  # Agregar el embedding único\n",
        "\n",
        "        # Asignar la lista de embeddings únicos al SKU en el nuevo diccionario\n",
        "        embeddings_no_duplicados[sku] = embeddings_unicos\n",
        "\n",
        "    return embeddings_no_duplicados\n",
        "\n",
        "# Uso de la función para obtener embeddings sin duplicados\n",
        "embeddings_sin_duplicados = obtener_embeddings_no_duplicados(embeddings_dict, duplicados_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHdQZyvYfWjQ"
      },
      "source": [
        "# Re-revisar duplicados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjzbLiijr_K2"
      },
      "source": [
        "# Upload to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DJG7-iZjr-ot"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=\"not the api key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xjjFWQxbsWJI"
      },
      "outputs": [],
      "source": [
        "index_name = \"google-fraud-detection-v2\"\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=512, # Replace with your model dimensions\n",
        "    metric=\"cosine\", # Replace with your model metric\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NN-Lty3AyO4M"
      },
      "outputs": [],
      "source": [
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jxXhwTN2yoYK"
      },
      "outputs": [],
      "source": [
        "# Prepare embeddings for upload as a dictionary\n",
        "embeddings_to_upload = [\n",
        "    {\"id\": f\"{sku}_{i}\", \"values\": embedding.detach().cpu().numpy().flatten().tolist()}\n",
        "    for sku, embeddings in embeddings_sin_duplicados.items()\n",
        "    for i, embedding in enumerate(embeddings)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssb9Pif8y9nE",
        "outputId": "6c1b98f8-c7a8-4620-f93b-4dbff0fc92fc"
      },
      "outputs": [],
      "source": [
        "# Define el tamaño del lote\n",
        "batch_size = 100  # Puedes ajustar este valor según la cantidad de datos y el límite de tamaño\n",
        "\n",
        "# Divide los embeddings en lotes más pequeños\n",
        "for i in range(0, len(embeddings_to_upload), batch_size):\n",
        "    batch = embeddings_to_upload[i:i + batch_size]\n",
        "    # Sube cada lote individualmente a Pinecone\n",
        "    index.upsert(vectors=batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de vectores en el índice: 6865\n"
          ]
        }
      ],
      "source": [
        "# Verifica las estadísticas del índice\n",
        "index_stats = index.describe_index_stats()\n",
        "\n",
        "# Imprime el número total de vectores en el índice\n",
        "print(\"Total de vectores en el índice:\", index_stats['total_vector_count'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
